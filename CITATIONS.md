# ðŸ“š Citations

This project builds on the work of many researchers and organizations in the fields of natural language processing, information retrieval, and large language models.

Please cite the following foundational papers and tools if you use this repo in academic work:

---

## ðŸ”¬ Foundational Research

### Attention & Transformers

* Vaswani et al., 2017 â€” *Attention is All You Need*
  [`https://arxiv.org/abs/1706.03762`](https://arxiv.org/abs/1706.03762)

### Tokenization & Embeddings

* Sennrich et al., 2016 â€” *Neural Machine Translation of Rare Words with Subword Units*
  [`https://arxiv.org/abs/1508.07909`](https://arxiv.org/abs/1508.07909)

### Retrieval-Augmented Generation

* Lewis et al., 2020 â€” *Retrieval-Augmented Generation for Knowledge-Intensive NLP Tasks*
  [`https://arxiv.org/abs/2005.11401`](https://arxiv.org/abs/2005.11401)

---

## ðŸ›  Libraries & Frameworks

* OpenAI GPT models â€” [`https://openai.com/research`](https://openai.com/research)
* Hugging Face Transformers â€” [`https://github.com/huggingface/transformers`](https://github.com/huggingface/transformers)
* FAISS by Facebook AI â€” [`https://github.com/facebookresearch/faiss`](https://github.com/facebookresearch/faiss)
* Streamlit â€” [`https://streamlit.io`](https://streamlit.io)

---

## ðŸ—£ Socratic Prompts for Deeper Study

* What assumptions does RAG make about relevance and trust?
* How do different tokenization schemes affect prompt length and semantics?
* In what ways do context order and format influence attention patterns?
* How can context engineering improve factual grounding without fine-tuning?

---

If you use this project in a publication, please include a link to the repository and cite the foundational works above. Thank you!
